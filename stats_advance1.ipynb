{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics Advance-1\n",
    "1. Explain the properties of the F-distribution. \n",
    "\n",
    "*The F-distribution is a continuous probability distribution that arises frequently in statistical tests, particularly in the analysis of variance (ANOVA) and regression analysis. \n",
    "*It is used to compare variances by examining the ratio of two independent chi-squared distributions divided by their respective degrees of freedom.\n",
    "\n",
    "Key Properties of the F-Distribution:\n",
    "1.Non-Negativity:\n",
    "The F-distribution is only defined for non-negative values (ğ¹â‰¥0), as it represents a ratio of variances, which are inherently non-negative.\n",
    "\n",
    "2.Shape:\n",
    "The shape of the F-distribution is asymmetrical (positively skewed).\n",
    "As the degrees of freedom (ğ‘‘1 and ğ‘‘2) increase, the distribution becomes less skewed and approaches a normal distribution.\n",
    "\n",
    "3.Degrees of Freedom:\n",
    "The F-distribution depends on two parameters:\n",
    "ğ‘‘1(degrees of freedom for the numerator).\n",
    "ğ‘‘2(degrees of freedom for the denominator).\n",
    "Larger values of ğ‘‘1 or ğ‘‘2 lead to a more symmetric shape.\n",
    "\n",
    "4.Right-Skewed:\n",
    "The distribution has a long right tail.Larger values of ğ‘‘1\n",
    "and ğ‘‘2 reduce this skewness.\n",
    "\n",
    "5.Mean:\n",
    "The mean of the F-distribution exists only if ğ‘‘2>2 and is given by:\n",
    "Mean=ğ‘‘2/(ğ‘‘2âˆ’2)\n",
    "\n",
    "6.Mode:\n",
    "The mode of the F-distribution (where the curve reaches its peak) is:\n",
    "Mode=((ğ‘‘1âˆ’2)ğ‘‘2)/(ğ‘‘1(ğ‘‘2+2)) forÂ d1>2\n",
    "\n",
    "Variance:\n",
    "The variance of the F-distribution exists only if d2>4 and is given by:\n",
    "Variance =2ğ‘‘2^2(ğ‘‘1+ğ‘‘2âˆ’2)/(ğ‘‘1(ğ‘‘2âˆ’2)^2(ğ‘‘2âˆ’4)) \n",
    "\n",
    "Critical Values:\n",
    "*The F-distribution is used to determine critical values for hypothesis tests, such as ANOVA and tests for equality of variances.\n",
    "*Critical values are typically found in F-tables or calculated using statistical software.\n",
    "\n",
    "Right-Tailed Test:\n",
    "Most applications of the F-distribution involve right-tailed tests because the focus is often on comparing the ratios of variances.\n",
    "\n",
    "Applications of the F-Distribution:\n",
    "Analysis of Variance (ANOVA):\n",
    "Used to compare the means of multiple groups by analyzing the variances within and between groups.\n",
    "\n",
    "Hypothesis Testing for Variance:\n",
    "To test if two populations have equal variances (e.g., Levene's test).\n",
    "\n",
    "Regression Analysis:\n",
    "Used to test the overall significance of a regression model by comparing the explained variance to the unexplained variance.\n",
    "\n",
    "Example:\n",
    "Suppose we want to test if two independent samples have equal variances. The F-statistic is calculated as:\n",
    "ğ¹=(ğ‘ 1^2/ğ‘‘1)/(ğ‘ 2^2/ğ‘‘2)\n",
    "â€‹Where:\n",
    "ğ‘ 1^2,s2^2: Sample variances.\n",
    "ğ‘‘1,d2: Degrees of freedom for the samples.\n",
    "If the calculated ğ¹-value exceeds the critical ğ¹-value from the F-distribution table for the given ğ‘‘1 and ğ‘‘2, we reject the null hypothesis of equal variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
    "\n",
    "*The F-distribution is widely used in statistical tests that involve comparing variances or analyzing relationships between multiple groups or variables. \n",
    "*It is appropriate for these tests because it models the ratio of variances from independent random samples, accounting for differences in sample sizes.\n",
    "\n",
    "Types of Statistical Tests Using the F-Distribution\n",
    "1. Analysis of Variance (ANOVA):\n",
    "*To compare the means of three or more groups to determine if at least one mean is significantly different.\n",
    "*ANOVA partitions the total variance into between-group variance and within-group variance.\n",
    "*The F-statistic tests whether the variance between groups is significantly greater than the variance within groups.\n",
    "\n",
    "Formula:\n",
    "ğ¹=Between-groupÂ Variance/Within-groupÂ Variance\n",
    "\n",
    "Example:\n",
    "Comparing exam scores of students from three different teaching methods.\n",
    "\n",
    "2. Test for Equality of Variances:\n",
    "*To determine if two or more groups have equal variances.\n",
    "*The F-statistic is calculated as the ratio of variances from two independent samples:\n",
    "\n",
    "ğ¹=ğ‘ 1^2/ğ‘ 2^2\n",
    "where ğ‘ 1^2 and ğ‘ 2^2 are sample variances.\n",
    "\n",
    "*This is appropriate because the F-distribution models the distribution of variance ratios.\n",
    "\n",
    "Example:\n",
    "Testing whether two manufacturing processes produce items with similar variability.\n",
    "\n",
    "3. Regression Analysis:\n",
    "*To test the overall significance of a regression model.\n",
    "*The F-test compares the variance explained by the regression model (model variance) to the unexplained variance (error variance).\n",
    "\n",
    "Formula:\n",
    "ğ¹=ExplainedÂ Variance/UnexplainedÂ Variance\n",
    "â€‹\n",
    " *This tests if the predictor variables collectively explain a significant amount of variability in the outcome.\n",
    "\n",
    "Example:\n",
    "Testing whether advertising spending predicts sales.\n",
    "\n",
    "4. Multivariate Analysis of Variance (MANOVA):\n",
    "*To compare means across multiple dependent variables for two or more groups.\n",
    "*MANOVA extends ANOVA to multiple dependent variables, with the F-distribution testing the overall model fit.\n",
    "\n",
    "Example:\n",
    "Comparing student performance across multiple subjects based on teaching methods.\n",
    "\n",
    "5. Nested Models (Likelihood Ratio Test in Regression):\n",
    "*To compare the goodness-of-fit between a full model (with more parameters) and a reduced model (with fewer parameters).\n",
    "*The F-statistic evaluates whether adding parameters significantly improves the model's fit.\n",
    "\n",
    "*The F-distribution is inherently designed to compare variances, which is central to many of these tests.\n",
    "*The two sets of degrees of freedom (ğ‘‘1 for the numerator, ğ‘‘2 for the denominator) account for sample sizes and ensure valid comparisons.\n",
    "*Most tests focus on whether observed variance ratios exceed critical thresholds, aligning with the right-skewed shape of the F-distribution.\n",
    "*The F-distribution accommodates small and large sample sizes, making it versatile for a range of tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
    "\n",
    "\n",
    "*The F-test is used to compare the variances of two populations. *For the results of the F-test to be valid, several key assumptions must be satisfied. \n",
    "*These assumptions ensure that the test statistic follows an F-distribution under the null hypothesis.\n",
    "\n",
    "Key Assumptions for the F-Test\n",
    "Independence of Samples:\n",
    "The two samples must be independent of each other. \n",
    "*This means the data in one sample should not influence or be related to the data in the other sample.\n",
    "Example: Comparing the variance of exam scores between two separate classes.\n",
    "\n",
    "Normality of Populations:\n",
    "*Both populations being compared must follow a normal distribution.\n",
    "*The F-test is sensitive to deviations from normality. Non-normal data can lead to incorrect conclusions, especially if sample sizes are small.\n",
    "Tip: If normality is questionable, consider using non-parametric alternatives like Leveneâ€™s test or Bartlettâ€™s test.\n",
    "\n",
    "Ratio of Variances:\n",
    "*The test statistic is a ratio of sample variances:\n",
    "ğ¹=ğ‘ 1^2/ğ‘ 2^2 \n",
    "where ğ‘ 1^2 and ğ‘ 2^2 are the variances of the two samples.\n",
    "*ğ‘ 1^2 is typically the larger variance, but not a strict requirement as long as the null hypothesis specifies equal variances.\n",
    "\n",
    "Random Sampling:\n",
    "The data in each sample must be collected through random sampling to ensure the validity and generalizability of the test results.\n",
    "\n",
    "Homogeneity of Variances (Optional for Two-Sample Tests):\n",
    "While the F-test checks for equal variances, some applications (like ANOVA) assume that the populations have equal variances before proceeding with further analysis.\n",
    "\n",
    "Positive Variances:\n",
    "The variances being compared must be positive since the ratio involves squared deviations.\n",
    "\n",
    "Null and Alternative Hypotheses\n",
    "Null Hypothesis (ğ»0): The two populations have equal variances:\n",
    "ğœ1^2=ğœ2^2\n",
    "Alternative Hypothesis (ğ»ğ‘): The variances are not equal:\n",
    "ğœ1^2â‰ ğœ2^2\n",
    "\n",
    "â€‹Sensitivity to Assumptions\n",
    "Normality: The F-test is particularly sensitive to departures from normality. If the data are not normally distributed, the test can become unreliable.\n",
    "Sample Size: For small sample sizes, deviations from normality and independence can significantly affect the results.\n",
    "\n",
    "Example\n",
    "Suppose we want to compare the variances of heights in two different groups:\n",
    "Group A:[170,165,180,175,172]\n",
    "Group B: [160,158,162,165,163]\n",
    "Before performing the F-test:\n",
    "*Ensure both groups' data are randomly sampled.\n",
    "*Verify the assumption of normality using visual checks (e.g., histograms) or formal tests (e.g., Shapiro-Wilk test).\n",
    "*Confirm that the samples are independent.\n",
    "*If these assumptions are met, proceed with the F-test to evaluate whether the variances of the two groups differ significantly.\n",
    "\n",
    "Alternatives if Assumptions Are Violated\n",
    "Leveneâ€™s Test:\n",
    "Robust to non-normality and tests for equality of variances.\n",
    "Brown-Forsythe Test:\n",
    "Similar to Leveneâ€™s test but uses the median instead of the mean, offering greater robustness to outliers.\n",
    "Transformations:\n",
    "Apply data transformations (e.g., logarithmic) to address issues with normality or skewness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the purpose of ANOVA, and how does it differ from a t-test? \n",
    "\n",
    "*ANOVA (Analysis of Variance) and the t-test are both statistical methods used to compare means, but they differ in scope and purpose. \n",
    "\n",
    "*ANOVA is used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others. \n",
    "\n",
    "*ANOVA tests for overall differences among group means by examining variability within groups (variability due to random error) and between groups (variability due to group differences).\n",
    "\n",
    "*It provides a single test statistic (the F-statistic) to evaluate these differences.\n",
    "\n",
    "*It avoids the increased risk of Type I errors that would result from conducting multiple t-tests between pairs of groups.\n",
    "\n",
    "*The t-test is used to compare the means of two groups to determine if they are significantly different. \n",
    "\n",
    "*For comparing two independent groups.\n",
    "\n",
    "*For comparing means within the same group at different times or under different conditions.\n",
    "\n",
    "*It's simpler than ANOVA and is limited to comparing two groups at a time.\n",
    "\n",
    "*Use a t-test if you are comparing exactly two groups.\n",
    "\n",
    "*Use ANOVA if you are comparing three or more groups or if you want a single test to evaluate multiple group comparisons.\n",
    "\n",
    "*If ANOVA indicates a significant difference, post-hoc tests (e.g., Tukey's HSD) can be used to determine which specific groups differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
    "\n",
    "*Problem with Multiple t-tests: Each t-test has its own risk of a Type I error (false positive), typically 5% for a ğ‘-value of 0.05. \n",
    "\n",
    "*If you perform multiple t-tests, the cumulative probability of making at least one Type I error increases significantly.\n",
    "\n",
    "*For example, comparing 3 groups requires 3 pairwise t-tests, and for 4 groups, it requires 6 t-tests.\n",
    "\n",
    "*The probability of at least one Type I error across these tests is given by 1âˆ’(1âˆ’ğ›¼)ğ‘˜, where ğ‘˜ is the number of comparisons.\n",
    "\n",
    "*Solution with ANOVA: One-way ANOVA performs a single test to determine if there are any significant differences among group means, maintaining the overall error rate at ğ›¼ (e.g., 0.05).\n",
    "\n",
    "*Multiple t-tests: When comparing ğ‘” groups, you need (ğ‘” 2)=ğ‘”(ğ‘”âˆ’1)/2 pairwise t-tests. \n",
    "\n",
    "*For example:\n",
    "\n",
    "3 groups: 3 t-tests\n",
    "4 groups: 6 t-tests\n",
    "5 groups: 10 t-tests\n",
    "\n",
    "*As the number of groups increases, the number of t-tests grows exponentially, making the process inefficient.\n",
    "\n",
    "*ANOVA: Requires only one test regardless of the number of groups, making it computationally and practically more efficient.\n",
    "\n",
    "*Multiple t-tests: Can only compare groups in pairs, so they do not provide an overall picture of the group differences.\n",
    "\n",
    "*ANOVA: Tests whether at least one group mean differs from the others by evaluating all groups simultaneously, giving a more holistic view.\n",
    "\n",
    "*You have one independent variable (factor) with three or more levels (groups) and want to determine if the dependent variable differs across the groups.\n",
    "\n",
    "Examples:\n",
    "*Testing the effectiveness of 3 different diets on weight loss.\n",
    "\n",
    "*Comparing test scores among 4 teaching methods.\n",
    "\n",
    "*Evaluating the customer satisfaction levels across 5 store branches.\n",
    "\n",
    "*Controls the overall Type I error rate.\n",
    "\n",
    "*Provides a single, unified test for comparing multiple groups.\n",
    "\n",
    "*If ANOVA finds significant differences, you can perform post-hoc tests (e.g., Tukey's HSD) to identify which specific groups differ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.How does this partitioning contribute to the calculation of the F-statistic?\n",
    "\n",
    "*In ANOVA (Analysis of Variance), the total variability in the data is partitioned into two components: between-group variance and within-group variance. \n",
    "\n",
    "*This partitioning forms the basis for calculating the F-statistic, which determines whether there are significant differences among group means. \n",
    "\n",
    "1. Partitioning Variance\n",
    "The total variance in the dataset is represented by the total sum of squares (ğ‘†ğ‘† Total ):\n",
    "ğ‘†ğ‘†Total=âˆ‘ i=1 to ğ‘ (ğ‘Œğ‘–âˆ’ ğ‘ŒË‰overall)^2\n",
    "Where:\n",
    "ğ‘Œğ‘–: Individual data points.\n",
    "ğ‘ŒË‰overall: Overall mean of all data points.\n",
    "ğ‘ : Total number of observations.\n",
    "\n",
    "This total variance is split into two components:\n",
    "\n",
    "a. Between-Group Variance (Explained Variance)\n",
    "This reflects the variability due to differences between the group means and the overall mean. It is calculated as the sum of squares between groups (ğ‘†ğ‘†Between):\n",
    "\n",
    "ğ‘†ğ‘† Between= âˆ‘ ğ‘—=1 to ğ‘˜ ğ‘› ğ‘—(ğ‘ŒË‰ğ‘—âˆ’ğ‘ŒË‰overall)^2\n",
    "Where:\n",
    "ğ‘˜: Number of groups.\n",
    "ğ‘›ğ‘—: Number of observations in group ğ‘—.\n",
    "ğ‘ŒË‰ğ‘—: Mean of group ğ‘—.\n",
    "\n",
    "b. Within-Group Variance (Unexplained Variance)\n",
    "This reflects the variability within each group, due to random error or individual differences. It is calculated as the sum of squares within groups (ğ‘†ğ‘† Within):\n",
    "\n",
    "ğ‘†ğ‘† Within= âˆ‘ ğ‘—=1 to ğ‘˜ âˆ‘ ğ‘–=1 to ğ‘›ğ‘— (ğ‘Œğ‘–ğ‘—âˆ’ğ‘ŒË‰ğ‘—)^2\n",
    "Where:\n",
    "ğ‘Œğ‘–ğ‘—: Individual data points in group ğ‘—.\n",
    "ğ‘ŒË‰ğ‘—: Mean of group j.\n",
    "Thus:\n",
    "ğ‘†ğ‘† Total = ğ‘†ğ‘† Between + ğ‘†ğ‘† Within\n",
    "â€‹\n",
    "*To standardize the variance, the sums of squares are divided by their respective degrees of freedom:\n",
    "\n",
    "Mean Square Between (ğ‘€ğ‘† Between):\n",
    "ğ‘€ğ‘† Between= ğ‘†ğ‘† Between / df Between \n",
    "\n",
    "Where ğ‘‘ğ‘“ Between =ğ‘˜âˆ’1 (number of groups minus 1).\n",
    "\n",
    "Mean Square Within (ğ‘€ğ‘† Within):\n",
    "ğ‘€ğ‘† Within = ğ‘†ğ‘† Within /ğ‘‘ğ‘“ Within\n",
    "Where ğ‘‘ğ‘“ Within = ğ‘âˆ’ğ‘˜ (total observations minus number of groups).\n",
    "\n",
    "*The F-statistic is the ratio of the between-group variance to the within-group variance:\n",
    "\n",
    "ğ¹=ğ‘€ğ‘† Between / ğ‘€ğ‘† Within\n",
    "\n",
    "Interpretation:\n",
    "*A large ğ¹-value indicates that the variance between groups is much larger than the variance within groups, suggesting significant differences among group means.\n",
    "\n",
    "*A small ğ¹-value indicates that the variance within groups dominates, suggesting no significant differences among group means.\n",
    "\n",
    "*The partitioning of variance isolates the effect of the independent variable (between-group variance) from random variability (within-group variance).\n",
    "\n",
    "*The F-statistic leverages this partitioning to test the null hypothesis that all group means are equal (ğ»0 : ğœ‡1 = ğœ‡2 = â‹¯ = ğœ‡ğ‘˜).\n",
    "\n",
    "*If ğ¹-statistic is large and significant (based on the F-distribution and a chosen ğ›¼-level), the null hypothesis is rejected, indicating that at least one group mean differs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
    "\n",
    "*The classical (frequentist) approach to ANOVA and the Bayesian approach differ fundamentally in their philosophical underpinnings, how they handle uncertainty, and how they approach parameter estimation and hypothesis testing. \n",
    "\n",
    "1. Handling Uncertainty\n",
    "Frequentist Approach:\n",
    "*Uncertainty is managed through p-values and confidence intervals.\n",
    "\n",
    "*The data is assumed to be a random sample, and probabilities are interpreted as long-run frequencies of repeated sampling.\n",
    "\n",
    "*Hypothesis testing focuses on rejecting or failing to reject the null hypothesis (ğ»0).\n",
    "\n",
    "Bayesian Approach:\n",
    "*Uncertainty is represented using probability distributions (posterior distributions) that reflect degrees of belief about parameters after observing the data.\n",
    "\n",
    "*Bayes' theorem combines prior beliefs (prior distribution) with the likelihood of the observed data to produce the posterior distribution.\n",
    "\n",
    "2. Parameter Estimation\n",
    "Frequentist Approach:\n",
    "*Parameters (e.g., group means) are treated as fixed but unknown quantities.\n",
    "*Estimates are based purely on the observed data and are often point estimates (e.g., means, variances).\n",
    "*Variance is decomposed into between-group variance and within-group variance to calculate the F-statistic.\n",
    "\n",
    "Bayesian Approach:\n",
    "*Parameters are treated as random variables with probability distributions.\n",
    "*Prior distributions represent beliefs about the parameters before observing data.\n",
    "*Posterior distributions integrate prior beliefs with the observed data, providing richer insights (e.g., probability intervals for parameters instead of point estimates).\n",
    "\n",
    "3. Hypothesis Testing\n",
    "Frequentist Approach:\n",
    "\n",
    "*Tests a null hypothesis (ğ»0) that all group means are equal (ğœ‡1 = ğœ‡2 = â‹¯ = ğœ‡ğ‘˜).\n",
    "\n",
    "*Relies on the F-statistic to determine if the variance between groups is significantly larger than within groups.\n",
    "\n",
    "*Provides a ğ‘-value to quantify the probability of observing the data (or more extreme) under ğ»0. If ğ‘<ğ›¼, ğ»0 is rejected.\n",
    "\n",
    "Bayesian Approach:\n",
    "*Does not rely on ğ»0 in the same way. Instead, it evaluates the relative credibility of competing models or hypotheses using posterior probabilities.\n",
    "\n",
    "*Focuses on computing the posterior probability of different hypotheses or models, often quantified using the Bayes Factor:\n",
    "BayesÂ Factor = P(dataÂ |Â ModelÂ 1) / P(dataÂ |Â ModelÂ 2)\n",
    "\n",
    "â€‹*Allows direct probability statements about hypotheses, e.g., \"The probability that ğœ‡1>ğœ‡2 is 0.85.\"\n",
    "\n",
    "4. Interpretation\n",
    "Frequentist Approach:\n",
    "*Results are interpreted in the context of the null hypothesis. A ğ‘-value less than ğ›¼ suggests evidence against ğ»0, but it does not provide the probability of ğ»0 being true.\n",
    "\n",
    "*Confidence intervals provide a range of plausible values for a \n",
    "parameter but are not probability intervals.\n",
    "\n",
    "Bayesian Approach:\n",
    "*Results are interpreted in terms of posterior probabilities, which directly answer questions like \"What is the probability of ğ»0 being true?\"\n",
    "*Credible intervals are derived from the posterior distribution and represent the range where the parameter is likely to fall with a given probability.\n",
    "\n",
    "Frequentist Approach:\n",
    "*Useful when prior information is unavailable or unreliable.\n",
    "*Provides well-established methods for hypothesis testing with simpler computational requirements.\n",
    "*Standard for many fields due to its simplicity and familiarity.\n",
    "\n",
    "Bayesian Approach:\n",
    "*Preferred when prior information is available or when uncertainty in parameters needs to be explicitly quantified.\n",
    "*Better suited for complex models, smaller datasets, or when probabilistic interpretations of hypotheses are desired.\n",
    "*Can answer more nuanced questions, such as the probability of specific parameter relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8. Question: You have two sets of data representing the incomes of two different professions1\n",
    " Profession A: [48, 52, 55, 60, 62]\n",
    " Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
    "incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
    "\n",
    "\n",
    "Results\n",
    "Using the provided data:\n",
    "\n",
    "F-statistic: 2.09\n",
    "p-value: 0.493\n",
    "\n",
    "Interpretation\n",
    "The null hypothesis (ğ»0) assumes that the variances of incomes for the two professions are equal.\n",
    "\n",
    "With a p-value of 0.493, which is greater than a common significance level (e.g., ğ›¼ = 0.05), we fail to reject the null hypothesis.\n",
    "\n",
    "Conclusion: There is no significant evidence to suggest that the variances of the incomes for Profession A and Profession B are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(2.089171974522293), np.float64(0.49304859900533904))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "profession_A = [48, 52, 55, 60, 62]\n",
    "profession_B = [45, 50, 55, 52, 47]\n",
    "\n",
    "\n",
    "var_A = np.var(profession_A, ddof=1)  \n",
    "var_B = np.var(profession_B, ddof=1)  \n",
    "\n",
    "\n",
    "F_statistic = var_A / var_B\n",
    "\n",
    "\n",
    "df_A = len(profession_A) - 1\n",
    "df_B = len(profession_B) - 1\n",
    "\n",
    "\n",
    "p_value = 2 * min(\n",
    "    stats.f.cdf(F_statistic, df_A, df_B),\n",
    "    1 - stats.f.cdf(F_statistic, df_A, df_B)\n",
    ")\n",
    "\n",
    "F_statistic, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
    "average heights between three different regions with the following data1\n",
    " Region A: [160, 162, 165, 158, 164]\n",
    " Region B: [172, 175, 170, 168, 174]\n",
    " Region C: [180, 182, 179, 185, 183]\n",
    " Task: Write Python code to perform the one-way ANOVA and interpret the results.\n",
    " Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
    "\n",
    "Results\n",
    "With the given data:\n",
    "\n",
    "F-statistic: 67.87\n",
    "p-value: \n",
    "2.87Ã—10âˆ’7\n",
    "\n",
    "Interpretation\n",
    "Null Hypothesis (ğ»0): The average heights are the same across all three regions (ğœ‡ğ´ = ğœ‡ğµ= ğœ‡ğ¶).\n",
    "Alternative Hypothesis (ğ»1): At least one region has a different average height.\n",
    "The p-value is much smaller than a typical significance level (eg., ğ›¼=0.05).\n",
    "\n",
    "Conclusion: We reject the null hypothesis. This indicates that there are statistically significant differences in the average heights between the regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.87330316742101\n",
      "p-value: 2.8706641879370266e-07\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "region_A = [160, 162, 165, 158, 164]\n",
    "region_B = [172, 175, 170, 168, 174]\n",
    "region_C = [180, 182, 179, 185, 183]\n",
    "\n",
    "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
    "\n",
    "print(f\"F-statistic: {F_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
